{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "3l4tqcjscmc6gtxnyomo",
   "authorId": "3903690185867",
   "authorName": "LILYXSU",
   "authorEmail": "lilyxsu@gmail.com",
   "sessionId": "c8b9adf7-7c07-445d-a99d-ad9cd14d4404",
   "lastEditTime": 1760772358958
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "codeCollapsed": false
   },
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Function\nfrom dataclasses import dataclass\nimport numpy as np\nfrom typing import Dict, List, Tuple, Optional\nimport time\n\n@dataclass\nclass MinimalConfig:\n    n_embd: int = 64\n    n_layer: int = 3\n    n_head: int = 4\n    vocab_size: int = 256\n    block_size: int = 32\n    n_features: int = 96\n    sparsity_penalty: float = 1e-3\n    jump_threshold: float = 0.03\n    pre_act_penalty: float = 3e-6\n    bandwidth: float = 1.0\n    learning_rate: float = 1e-4\n    n_training_tokens: int = 100_000_000\n\nclass JumpReLUFunction(Function):\n    @staticmethod\n    def forward(ctx, x, threshold, bandwidth):\n        ctx.save_for_backward(x, threshold)\n        ctx.bandwidth = bandwidth\n        active = (x > threshold).float()\n        return F.relu(x - threshold), active\n    \n    @staticmethod\n    def backward(ctx, grad_output, grad_active):\n        x, threshold = ctx.saved_tensors\n        in_bandwidth = (torch.abs(x - threshold) <= ctx.bandwidth).float()\n        active = (x > threshold).float()\n        grad_x = grad_output * (active + (1 - active) * in_bandwidth)\n        grad_threshold = -grad_output * active\n        grad_threshold = grad_threshold.sum(dim=(0, 1))\n        return grad_x, grad_threshold, None\n\nclass JumpReLU(nn.Module):\n    def __init__(self, n_features, threshold=0.03, bandwidth=1.0):\n        super().__init__()\n        self.threshold = nn.Parameter(torch.full((n_features,), threshold))\n        self.bandwidth = bandwidth\n    \n    def forward(self, x):\n        return JumpReLUFunction.apply(x, self.threshold, self.bandwidth)\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.attn = nn.MultiheadAttention(\n            config.n_embd, config.n_head, dropout=0.0, batch_first=True\n        )\n        self.mlp = nn.Sequential(\n            nn.Linear(config.n_embd, 4 * config.n_embd),\n            nn.GELU(),\n            nn.Linear(4 * config.n_embd, config.n_embd)\n        )\n    \n    def forward(self, x, return_components=False, frozen_ln1_denom=None,\n                frozen_ln2_denom=None, frozen_attn_patterns=None):\n        residual_pre_attn = x\n        \n        if frozen_ln1_denom is not None:\n            mean = x.mean(dim=-1, keepdim=True)\n            x_norm1 = (x - mean) / frozen_ln1_denom\n            x_norm1 = x_norm1 * self.ln_1.weight + self.ln_1.bias\n            ln1_denom = frozen_ln1_denom\n        else:\n            x_norm1 = self.ln_1(x)\n            mean = x.mean(dim=-1, keepdim=True)\n            var = ((x - mean)**2).mean(dim=-1, keepdim=True)\n            ln1_denom = torch.sqrt(var + self.ln_1.eps)\n        \n        if frozen_attn_patterns is not None:\n            B, T, D = x_norm1.shape\n            # MultiheadAttention with average_attn_weights=True returns [B, T, T]\n            # MultiheadAttention with average_attn_weights=False returns [B, num_heads, T, T]\n            # But by default, PyTorch's MultiheadAttention returns averaged weights [B, T, T]\n            \n            # Apply attention using frozen patterns\n            if frozen_attn_patterns.dim() == 2:  # [T, T] - need to add batch dim\n                attn_weights_expanded = frozen_attn_patterns.unsqueeze(0).expand(B, -1, -1)\n                attn_out = torch.bmm(attn_weights_expanded, x_norm1)\n            elif frozen_attn_patterns.dim() == 3:  # [B, T, T] or [num_heads, T, T]\n                if frozen_attn_patterns.shape[0] == B:  # [B, T, T]\n                    attn_out = torch.bmm(frozen_attn_patterns, x_norm1)\n                elif frozen_attn_patterns.shape[0] == self.attn.num_heads:  # [num_heads, T, T]\n                    # Need to apply per-head attention\n                    head_dim = D // self.attn.num_heads\n                    v = x_norm1.view(B, T, self.attn.num_heads, head_dim).transpose(1, 2)\n                    attn_out = torch.zeros(B, self.attn.num_heads, T, head_dim, device=x.device)\n                    for h in range(self.attn.num_heads):\n                        v_head = v[:, h, :, :]\n                        attn_weights_h = frozen_attn_patterns[h, :, :].unsqueeze(0).expand(B, -1, -1)\n                        attn_out[:, h, :, :] = torch.bmm(attn_weights_h, v_head)\n                    attn_out = attn_out.transpose(1, 2).reshape(B, T, D)\n                else:  # Likely [1, T, T] - averaged weights with an extra dim\n                    attn_weights_expanded = frozen_attn_patterns.squeeze(0).unsqueeze(0).expand(B, -1, -1)\n                    attn_out = torch.bmm(attn_weights_expanded, x_norm1)\n            else:\n                raise ValueError(f\"Unexpected frozen_attn_patterns shape: {frozen_attn_patterns.shape}\")\n            \n            attn_weights = frozen_attn_patterns\n        else:\n            attn_out, attn_weights = self.attn(x_norm1, x_norm1, x_norm1)\n        \n        x = residual_pre_attn + attn_out\n        residual_pre_mlp = x\n        \n        if frozen_ln2_denom is not None:\n            mean = x.mean(dim=-1, keepdim=True)\n            x_norm2 = (x - mean) / frozen_ln2_denom\n            x_norm2 = x_norm2 * self.ln_2.weight + self.ln_2.bias\n            ln2_denom = frozen_ln2_denom\n        else:\n            x_norm2 = self.ln_2(x)\n            mean = x.mean(dim=-1, keepdim=True)\n            var = ((x - mean)**2).mean(dim=-1, keepdim=True)\n            ln2_denom = torch.sqrt(var + self.ln_2.eps)\n        \n        mlp_out = self.mlp(x_norm2)\n        x = residual_pre_mlp + mlp_out\n        \n        if return_components:\n            return x, {\n                'residual_pre_attn': residual_pre_attn,\n                'residual_pre_mlp': residual_pre_mlp,\n                'attn_out': attn_out,\n                'pre_mlp_norm': x_norm2,\n                'mlp_output': mlp_out,\n                'ln1_denom': ln1_denom,\n                'ln2_denom': ln2_denom,\n                'attn_weights': attn_weights,\n                'attn_norm': x_norm1\n            }\n        return x\n\nclass MinimalTransformer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.token_emb = nn.Embedding(config.vocab_size, config.n_embd)\n        self.pos_emb = nn.Embedding(config.block_size, config.n_embd)\n        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(config.n_layer)])\n        self.ln_f = nn.LayerNorm(config.n_embd)\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n    \n    def forward(self, input_ids, collect_activations=False,\n                frozen_ln1_denoms=None, frozen_ln2_denoms=None,\n                frozen_attn_patterns=None, frozen_lnf_denom=None):\n        B, T = input_ids.shape\n        token_embeddings = self.token_emb(input_ids)\n        pos = torch.arange(T, device=input_ids.device).unsqueeze(0)\n        pos_embeddings = self.pos_emb(pos)\n        x = token_embeddings + pos_embeddings\n        \n        if collect_activations:\n            activations = {\n                'embeddings': x,\n                'token_embeddings': token_embeddings,\n                'pos_embeddings': pos_embeddings,\n                'pre_mlp_norms': [],\n                'mlp_outputs': [],\n                'ln1_denoms': [],\n                'ln2_denoms': [],\n                'residual_pre_mlps': [],\n                'attn_weights': [],\n                'attn_norms': [],\n                'attn_outs': [],\n                'residual_pre_attns': []\n            }\n        \n        for i, block in enumerate(self.blocks):\n            frozen_ln1 = frozen_ln1_denoms[i] if frozen_ln1_denoms else None\n            frozen_ln2 = frozen_ln2_denoms[i] if frozen_ln2_denoms else None\n            frozen_attn = frozen_attn_patterns[i] if frozen_attn_patterns else None\n            \n            if collect_activations:\n                x, components = block(x, return_components=True,\n                                     frozen_ln1_denom=frozen_ln1,\n                                     frozen_ln2_denom=frozen_ln2,\n                                     frozen_attn_patterns=frozen_attn)\n                activations['pre_mlp_norms'].append(components['pre_mlp_norm'])\n                activations['mlp_outputs'].append(components['mlp_output'])\n                activations['ln1_denoms'].append(components['ln1_denom'])\n                activations['ln2_denoms'].append(components['ln2_denom'])\n                activations['residual_pre_mlps'].append(components['residual_pre_mlp'])\n                activations['attn_weights'].append(components['attn_weights'])\n                activations['attn_norms'].append(components['attn_norm'])\n                activations['attn_outs'].append(components['attn_out'])\n                activations['residual_pre_attns'].append(components['residual_pre_attn'])\n            else:\n                x = block(x, frozen_ln1_denom=frozen_ln1,\n                         frozen_ln2_denom=frozen_ln2,\n                         frozen_attn_patterns=frozen_attn)\n        \n        if frozen_lnf_denom is not None:\n            mean = x.mean(dim=-1, keepdim=True)\n            x = (x - mean) / frozen_lnf_denom\n            x = x * self.ln_f.weight + self.ln_f.bias\n            if collect_activations:\n                activations['lnf_denom'] = frozen_lnf_denom\n        else:\n            if collect_activations:\n                mean = x.mean(dim=-1, keepdim=True)\n                var = ((x - mean)**2).mean(dim=-1, keepdim=True)\n                activations['lnf_denom'] = torch.sqrt(var + self.ln_f.eps)\n            x = self.ln_f(x)\n        \n        logits = self.lm_head(x)\n        \n        return (logits, activations) if collect_activations else logits\n\nclass CrossLayerTranscoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.n_features = config.n_features\n        self.n_layers = config.n_layer\n        \n        self.encoders = nn.ModuleList([\n            nn.Linear(config.n_embd, self.n_features) for _ in range(self.n_layers)\n        ])\n        \n        self.activations = nn.ModuleList([\n            JumpReLU(self.n_features, config.jump_threshold, config.bandwidth)\n            for _ in range(self.n_layers)\n        ])\n        \n        self.decoders = nn.ModuleDict()\n        for k in range(self.n_layers):\n            for j in range(k, self.n_layers):\n                self.decoders[f\"{k}_to_{j}\"] = nn.Linear(self.n_features, config.n_embd, bias=False)\n        \n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        for encoder in self.encoders:\n            bound = 1 / np.sqrt(self.n_features)\n            nn.init.uniform_(encoder.weight, -bound, bound)\n            nn.init.zeros_(encoder.bias)\n        \n        bound = 1 / np.sqrt(self.n_layers * self.config.n_embd)\n        for decoder in self.decoders.values():\n            nn.init.uniform_(decoder.weight, -bound, bound)\n    \n    def forward(self, pre_mlp_norms, compute_jacobian=False, normalize_inputs=True):\n        all_features, all_active, all_pre_acts = [], [], []\n        \n        normalized_inputs = []\n        for k in range(self.n_layers):\n            if normalize_inputs:\n                norm_input = F.normalize(pre_mlp_norms[k], p=2, dim=-1, eps=1e-12)\n                norm_input = norm_input * np.sqrt(self.config.n_embd)\n            else:\n                norm_input = pre_mlp_norms[k]\n            normalized_inputs.append(norm_input)\n        \n        for k in range(self.n_layers):\n            pre_act = self.encoders[k](normalized_inputs[k])\n            features, active = self.activations[k](pre_act)\n            all_pre_acts.append(pre_act)\n            all_features.append(features)\n            all_active.append(active)\n        \n        reconstructions = []\n        for j in range(self.n_layers):\n            reconstruction = torch.zeros_like(pre_mlp_norms[0])\n            for k in range(j + 1):\n                if compute_jacobian:\n                    features_k = all_features[k].detach() * all_active[k]\n                else:\n                    features_k = all_features[k]\n                reconstruction += self.decoders[f\"{k}_to_{j}\"](features_k)\n            reconstructions.append(reconstruction)\n        \n        return reconstructions, all_features, all_active, all_pre_acts\n\nclass CLTTrainer:\n    def __init__(self, base_model, clt, config):\n        self.base_model = base_model\n        self.clt = clt\n        self.config = config\n        \n        total_params = sum(p.numel() for p in clt.parameters())\n        lr_scale = 1.0 / np.sqrt(total_params)\n        self.optimizer = torch.optim.AdamW(\n            clt.parameters(),\n            lr=config.learning_rate * lr_scale,\n            betas=(0.9, 0.999)\n        )\n        \n        self.step = 0\n        self.total_steps = int(2000 * np.power(config.n_features / 96, 0.8))\n    \n    def compute_loss(self, mlp_outputs, reconstructions, features, pre_acts):\n        normalized_targets = []\n        for j in range(len(mlp_outputs)):\n            norm_target = F.normalize(mlp_outputs[j], p=2, dim=-1, eps=1e-12)\n            norm_target = norm_target * np.sqrt(self.config.n_embd)\n            normalized_targets.append(norm_target)\n        \n        normalized_recons = []\n        for j in range(len(reconstructions)):\n            norm_recon = F.normalize(reconstructions[j], p=2, dim=-1, eps=1e-12)\n            norm_recon = norm_recon * np.sqrt(self.config.n_embd)\n            normalized_recons.append(norm_recon)\n        \n        recon_loss = sum(F.mse_loss(r, t) for r, t in zip(normalized_recons, normalized_targets)) / len(mlp_outputs)\n        \n        sparsity_loss = sum(torch.tanh(f).mean() for f in features) / len(features)\n        \n        ramp_factor = min(1.0, self.step / self.total_steps)\n        sparsity_weight = self.config.sparsity_penalty * ramp_factor\n        \n        pre_act_loss = sum(F.relu(-p).mean() for p in pre_acts) / len(pre_acts)\n        \n        total = recon_loss + sparsity_weight * sparsity_loss + self.config.pre_act_penalty * pre_act_loss\n        \n        l0 = sum([(f > 0).float().mean().item() for f in features]) / len(features)\n        \n        return total, recon_loss.item(), sparsity_loss.item(), l0\n    \n    def train_step(self, batch):\n        self.step += 1\n        \n        with torch.no_grad():\n            _, activations = self.base_model(batch, collect_activations=True)\n        \n        reconstructions, features, active_masks, pre_acts = self.clt(activations['pre_mlp_norms'])\n        \n        loss, recon, sparsity, l0 = self.compute_loss(\n            activations['mlp_outputs'], reconstructions, features, pre_acts\n        )\n        \n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        \n        return {'loss': loss.item(), 'recon': recon, 'sparsity': sparsity, 'l0': l0}\n\nclass AttributionGraph:\n    def __init__(self, base_model, clt):\n        self.base_model = base_model\n        self.clt = clt\n        self.n_layers = base_model.config.n_layer\n        self.d_model = base_model.config.n_embd\n        self.n_heads = base_model.config.n_head\n        self.coactivation_counts = {}\n        self.feature_activation_counts = {}\n    \n    def compute_virtual_weights(self, k: int, i: int, j: int) -> torch.Tensor:\n        sum_dec = torch.zeros(self.d_model)\n        for l in range(k, j):\n            sum_dec += self.clt.decoders[f\"{k}_to_{l}\"].weight[:, i]\n        \n        virtual_weights = torch.matmul(sum_dec, self.clt.encoders[j].weight.T)\n        return virtual_weights\n    \n    def compute_twera(self, virtual_weight: float, source_key: str, target_key: str) -> float:\n        coact_count = self.coactivation_counts.get((source_key, target_key), 0)\n        source_count = self.feature_activation_counts.get(source_key, 1)\n        \n        p_coactive = coact_count / max(source_count, 1)\n        \n        return virtual_weight * p_coactive\n    \n    def update_coactivation_stats(self, features, active_masks):\n        for layer_k in range(self.n_layers):\n            active_k = active_masks[layer_k] > 0\n            for idx_k in torch.where(active_k.flatten())[0]:\n                key_k = f\"L{layer_k}_F{idx_k.item()}\"\n                self.feature_activation_counts[key_k] = self.feature_activation_counts.get(key_k, 0) + 1\n                \n                for layer_j in range(layer_k + 1, self.n_layers):\n                    active_j = active_masks[layer_j] > 0\n                    for idx_j in torch.where(active_j.flatten())[0]:\n                        key_j = f\"L{layer_j}_F{idx_j.item()}\"\n                        if active_k.flatten()[idx_k] and active_j.flatten()[idx_j]:\n                            pair = (key_k, key_j)\n                            self.coactivation_counts[pair] = self.coactivation_counts.get(pair, 0) + 1\n    \n    def compute_jacobian_attribution(self, source_vec: torch.Tensor, target_vec: torch.Tensor,\n                                    acts: Dict, layer_source: int, layer_target: int,\n                                    frozen_denoms: Dict) -> float:\n        source_injection = source_vec.clone().detach().requires_grad_(True)\n        \n        with torch.enable_grad():\n            x = source_injection\n            \n            for l in range(layer_source, min(layer_target, self.n_layers)):\n                residual = x\n                \n                if l < len(acts['attn_outs']):\n                    x = residual + acts['attn_outs'][l][0, -1].detach()\n                \n                if l < layer_target:\n                    x = residual + x * 0.1\n            \n            output = torch.dot(x.flatten(), target_vec.flatten())\n            \n            grad = torch.autograd.grad(output, source_injection,\n                                      retain_graph=False,\n                                      create_graph=False)[0]\n            \n            grad = grad.detach()\n        \n        return torch.dot(source_vec.detach(), grad).item()\n    \n    def compute_indirect_influence(self, adjacency_matrix: torch.Tensor) -> torch.Tensor:\n        A = torch.abs(adjacency_matrix)\n        \n        row_sums = A.sum(dim=1, keepdim=True)\n        row_sums = torch.clamp(row_sums, min=1e-8)\n        A_norm = A / row_sums\n        \n        I = torch.eye(A.shape[0])\n        try:\n            influence = torch.linalg.inv(I - A_norm) - I\n        except:\n            influence = A_norm.clone()\n            A_power = A_norm.clone()\n            for _ in range(10):\n                A_power = torch.matmul(A_power, A_norm)\n                influence += A_power\n                if A_power.abs().max() < 1e-6:\n                    break\n        \n        return influence\n    \n    def compute_full_graph(self, input_ids, target_pos=-1, use_twera=False):\n        device = input_ids.device\n        B, T = input_ids.shape\n        \n        with torch.no_grad():\n            _, acts_initial = self.base_model(input_ids, collect_activations=True)\n            frozen_ln1_denoms = acts_initial['ln1_denoms']\n            frozen_ln2_denoms = acts_initial['ln2_denoms']\n            frozen_lnf_denom = acts_initial.get('lnf_denom', None)\n            frozen_attn_patterns = acts_initial['attn_weights']\n            \n            logits, acts = self.base_model(\n                input_ids,\n                collect_activations=True,\n                frozen_ln1_denoms=frozen_ln1_denoms,\n                frozen_ln2_denoms=frozen_ln2_denoms,\n                frozen_attn_patterns=frozen_attn_patterns,\n                frozen_lnf_denom=frozen_lnf_denom\n            )\n            \n            reconstructions, features, active_masks, pre_acts = self.clt(\n                acts['pre_mlp_norms'], compute_jacobian=True\n            )\n        \n        self.update_coactivation_stats(features, active_masks)\n        \n        errors = []\n        for j in range(self.n_layers):\n            error = acts['mlp_outputs'][j] - reconstructions[j]\n            errors.append(error.detach())\n        \n        target_token = torch.argmax(logits[0, target_pos]).item()\n        unembed = self.base_model.lm_head.weight[target_token, :]\n        mean_unembed = self.base_model.lm_head.weight.mean(dim=0)\n        logit_v = unembed - mean_unembed\n        \n        token_id = input_ids[0, target_pos].item()\n        active_features = []\n        for layer in range(self.n_layers):\n            active_mask = active_masks[layer][0, target_pos] > 0\n            for idx in torch.where(active_mask)[0]:\n                active_features.append({\n                    'layer': layer,\n                    'index': idx.item(),\n                    'activation': features[layer][0, target_pos, idx].item()\n                })\n        \n        nodes = {\n            'embeddings': [{'type': 'token', 'position': target_pos, 'token_id': token_id}],\n            'features': active_features,\n            'errors': [{'layer': j, 'norm': torch.norm(errors[j][0, target_pos]).item()}\n                      for j in range(self.n_layers) if torch.norm(errors[j][0, target_pos]).item() > 0.01],\n            'attention_heads': [],\n            'logit': f\"logit_{target_token}\"\n        }\n        \n        # In compute_full_graph method, replace the attention_heads section (around line 495-510) with:\n        \n        for layer in range(self.n_layers):\n            attn_weights = frozen_attn_patterns[layer]\n            \n            # Handle different shapes of attention weights\n            if attn_weights.dim() == 4 and attn_weights.shape[1] == self.n_heads:\n                # [B, num_heads, T, T] - per-head weights\n                attn_weights = attn_weights[0]  # Take first batch\n                for head in range(self.n_heads):\n                    attn_pattern = attn_weights[head, target_pos, :].cpu().numpy()\n                    if np.max(attn_pattern) > 0.1:\n                        nodes['attention_heads'].append({\n                            'layer': layer,\n                            'head': head,\n                            'max_weight': float(np.max(attn_pattern)),\n                            'pattern': attn_pattern\n                        })\n            elif attn_weights.dim() == 3:\n                # [B, T, T] or [num_heads, T, T]\n                if attn_weights.shape[0] == self.n_heads:\n                    # [num_heads, T, T] - per-head weights without batch\n                    for head in range(self.n_heads):\n                        attn_pattern = attn_weights[head, target_pos, :].cpu().numpy()\n                        if np.max(attn_pattern) > 0.1:\n                            nodes['attention_heads'].append({\n                                'layer': layer,\n                                'head': head,\n                                'max_weight': float(np.max(attn_pattern)),\n                                'pattern': attn_pattern\n                            })\n                else:\n                    # [B, T, T] - averaged weights\n                    attn_pattern = attn_weights[0, target_pos, :].cpu().numpy()\n                    if np.max(attn_pattern) > 0.1:\n                        # Add as a single \"averaged\" attention head\n                        nodes['attention_heads'].append({\n                            'layer': layer,\n                            'head': 0,  # Use head 0 to represent averaged\n                            'max_weight': float(np.max(attn_pattern)),\n                            'pattern': attn_pattern\n                        })\n            elif attn_weights.dim() == 2:\n                # [T, T] - averaged weights without batch\n                attn_pattern = attn_weights[target_pos, :].cpu().numpy()\n                if np.max(attn_pattern) > 0.1:\n                    nodes['attention_heads'].append({\n                        'layer': layer,\n                        'head': 0,  # Use head 0 to represent averaged\n                        'max_weight': float(np.max(attn_pattern)),\n                        'pattern': attn_pattern\n                    })\n        \n        edges = []\n        \n        emb = acts['embeddings'][0, target_pos].detach()\n        direct_contrib = torch.dot(emb, logit_v).item()\n        if abs(direct_contrib) > 0.01:\n            edges.append({\n                'source': f\"emb_{token_id}\",\n                'target': f\"logit_{target_token}\",\n                'weight': direct_contrib,\n                'type': 'embedding_to_logit_direct'\n            })\n        \n        frozen_denoms = {\n            'ln1': frozen_ln1_denoms,\n            'ln2': frozen_ln2_denoms,\n            'lnf': frozen_lnf_denom,\n            'attn': frozen_attn_patterns\n        }\n        \n        for feat in active_features:\n            if feat['layer'] == 0:\n                enc_weight = self.clt.encoders[0].weight[feat['index'], :]\n                weight = self.compute_jacobian_attribution(\n                    emb, enc_weight, acts, 0, 0, frozen_denoms\n                ) * feat['activation']\n                if abs(weight) > 0.01:\n                    edges.append({\n                        'source': f\"emb_{token_id}\",\n                        'target': f\"L0_F{feat['index']}\",\n                        'weight': weight,\n                        'type': 'embedding_to_feature'\n                    })\n        \n        for s in active_features:\n            k, i, a_s = s['layer'], s['index'], s['activation']\n            source_key = f\"L{k}_F{i}\"\n            \n            for t in active_features:\n                j, m = t['layer'], t['index']\n                if j <= k:\n                    continue\n                \n                target_key = f\"L{j}_F{m}\"\n                \n                virtual_weights = self.compute_virtual_weights(k, i, j)\n                V_st = virtual_weights[m].item()\n                \n                if use_twera:\n                    edge_weight = self.compute_twera(a_s * V_st, source_key, target_key)\n                else:\n                    edge_weight = a_s * V_st\n                \n                if abs(edge_weight) > 0.01:\n                    edges.append({\n                        'source': source_key,\n                        'target': target_key,\n                        'weight': edge_weight,\n                        'type': 'feature_to_feature'\n                    })\n        \n        for s in active_features:\n            k, i, a_s = s['layer'], s['index'], s['activation']\n            \n            sum_dec = torch.zeros(self.d_model, device=device)\n            for l in range(k, self.n_layers):\n                decoder_weight = self.clt.decoders[f\"{k}_to_{l}\"].weight[:, i].detach()\n                sum_dec += decoder_weight\n            \n            weight = a_s * torch.dot(sum_dec, logit_v).item()\n            if abs(weight) > 0.01:\n                edges.append({\n                    'source': f\"L{k}_F{i}\",\n                    'target': f\"logit_{target_token}\",\n                    'weight': weight,\n                    'type': 'feature_to_logit'\n                })\n        \n        for error_info in nodes['errors']:\n            j = error_info['layer']\n            error_vec = errors[j][0, target_pos, :].detach()\n            \n            contrib = torch.dot(error_vec, logit_v).item()\n            if abs(contrib) > 0.01:\n                edges.append({\n                    'source': f\"error_L{j}\",\n                    'target': f\"logit_{target_token}\",\n                    'weight': contrib,\n                    'type': 'error_to_logit'\n                })\n            \n            for t in active_features:\n                if t['layer'] > j:\n                    enc_t = self.clt.encoders[t['layer']].weight[t['index'], :].detach()\n                    weight = torch.dot(error_vec, enc_t).item()\n                    if abs(weight) > 0.01:\n                        edges.append({\n                            'source': f\"error_L{j}\",\n                            'target': f\"L{t['layer']}_F{t['index']}\",\n                            'weight': weight,\n                            'type': 'error_to_feature'\n                        })\n        \n        for attn_node in nodes['attention_heads']:\n            layer = attn_node['layer']\n            head = attn_node['head']\n            \n            attn_out = acts['attn_outs'][layer][0, target_pos].detach()\n            head_contribution = attn_out / self.n_heads\n            \n            for feat in active_features:\n                if feat['layer'] > layer:\n                    enc_weight = self.clt.encoders[feat['layer']].weight[feat['index'], :].detach()\n                    weight = torch.dot(head_contribution, enc_weight).item()\n                    if abs(weight) > 0.01:\n                        edges.append({\n                            'source': f\"L{layer}_H{head}\",\n                            'target': f\"L{feat['layer']}_F{feat['index']}\",\n                            'weight': weight,\n                            'type': 'attention_to_feature'\n                        })\n            \n            contrib = torch.dot(head_contribution, logit_v).item()\n            if abs(contrib) > 0.01:\n                edges.append({\n                    'source': f\"L{layer}_H{head}\",\n                    'target': f\"logit_{target_token}\",\n                    'weight': contrib,\n                    'type': 'attention_to_logit'\n                })\n        \n        return {\n            'nodes': nodes,\n            'edges': edges,\n            'target_token': target_token,\n            'position': target_pos,\n            'frozen_denoms': frozen_denoms\n        }\n    \n    def prune_graph(self, graph: Dict, node_threshold: float = 0.8, edge_threshold: float = 0.98) -> Dict:\n        node_names = []\n        node_names.append(f\"emb_{graph['nodes']['embeddings'][0]['token_id']}\")\n        for feat in graph['nodes']['features']:\n            node_names.append(f\"L{feat['layer']}_F{feat['index']}\")\n        for err in graph['nodes']['errors']:\n            node_names.append(f\"error_L{err['layer']}\")\n        for attn in graph['nodes']['attention_heads']:\n            node_names.append(f\"L{attn['layer']}_H{attn['head']}\")\n        node_names.append(graph['nodes']['logit'])\n        \n        n_nodes = len(node_names)\n        adjacency = torch.zeros(n_nodes, n_nodes)\n        \n        node_to_idx = {name: i for i, name in enumerate(node_names)}\n        for edge in graph['edges']:\n            if edge['source'] in node_to_idx and edge['target'] in node_to_idx:\n                i, j = node_to_idx[edge['source']], node_to_idx[edge['target']]\n                adjacency[j, i] = abs(edge['weight'])\n        \n        influence = self.compute_indirect_influence(adjacency)\n        \n        logit_idx = node_to_idx[graph['nodes']['logit']]\n        logit_influence = influence[logit_idx, :]\n        \n        sorted_influence, sorted_indices = torch.sort(logit_influence, descending=True)\n        cumsum = torch.cumsum(sorted_influence, dim=0)\n        cumsum = cumsum / (cumsum[-1] + 1e-8)\n        n_keep = (cumsum <= node_threshold).sum().item() + 1\n        \n        top_indices = sorted_indices[:n_keep]\n        kept_nodes = set([node_names[i] for i in top_indices])\n        kept_nodes.add(graph['nodes']['logit'])\n        \n        kept_edges = [e for e in graph['edges']\n                     if e['source'] in kept_nodes and e['target'] in kept_nodes]\n        \n        kept_node_list = list(kept_nodes)\n        kept_node_to_idx = {name: i for i, name in enumerate(kept_node_list)}\n        n_kept = len(kept_node_list)\n        \n        kept_adjacency = torch.zeros(n_kept, n_kept)\n        edge_to_weight = {}\n        \n        for edge in kept_edges:\n            i = kept_node_to_idx[edge['source']]\n            j = kept_node_to_idx[edge['target']]\n            kept_adjacency[j, i] = abs(edge['weight'])\n            edge_to_weight[(edge['source'], edge['target'])] = abs(edge['weight'])\n        \n        kept_influence = self.compute_indirect_influence(kept_adjacency)\n        \n        logit_idx_kept = kept_node_to_idx[graph['nodes']['logit']]\n        node_scores = kept_influence[logit_idx_kept, :]\n        \n        edge_scores = []\n        for edge in kept_edges:\n            source_idx = kept_node_to_idx[edge['source']]\n            target_idx = kept_node_to_idx[edge['target']]\n            edge_score = kept_adjacency[target_idx, source_idx] * node_scores[target_idx]\n            edge_scores.append((edge, edge_score.item()))\n        \n        edge_scores.sort(key=lambda x: x[1], reverse=True)\n        total_score = sum(score for _, score in edge_scores)\n        \n        pruned_edges = []\n        cumulative_score = 0\n        for edge, score in edge_scores:\n            cumulative_score += score\n            pruned_edges.append(edge)\n            if cumulative_score / total_score >= edge_threshold:\n                break\n        \n        final_nodes = set()\n        for edge in pruned_edges:\n            final_nodes.add(edge['source'])\n            final_nodes.add(edge['target'])\n        final_nodes.add(graph['nodes']['logit'])\n        \n        pruned_nodes = {\n            'embeddings': [e for e in graph['nodes']['embeddings']\n                          if f\"emb_{e['token_id']}\" in final_nodes],\n            'features': [f for f in graph['nodes']['features']\n                        if f\"L{f['layer']}_F{f['index']}\" in final_nodes],\n            'errors': [e for e in graph['nodes']['errors']\n                      if f\"error_L{e['layer']}\" in final_nodes],\n            'attention_heads': [a for a in graph['nodes']['attention_heads']\n                               if f\"L{a['layer']}_H{a['head']}\" in final_nodes],\n            'logit': graph['nodes']['logit']\n        }\n        \n        return {\n            'nodes': pruned_nodes,\n            'edges': pruned_edges,\n            'target_token': graph['target_token'],\n            'position': graph['position'],\n            'frozen_denoms': graph.get('frozen_denoms', None)\n        }\n    \n    def group_into_supernodes(self, graph: Dict, similarity_threshold: float = 0.7) -> Dict:\n        features = graph['nodes']['features']\n        if len(features) < 2:\n            return graph\n        \n        groups = []\n        used = set()\n        \n        for i, feat_i in enumerate(features):\n            if i in used:\n                continue\n            \n            group = [feat_i]\n            used.add(i)\n            \n            layer_i = feat_i['layer']\n            idx_i = feat_i['index']\n            \n            dec_i = torch.zeros(self.d_model)\n            for l in range(layer_i, self.n_layers):\n                dec_i += self.clt.decoders[f\"{layer_i}_to_{l}\"].weight[:, idx_i]\n            \n            for j, feat_j in enumerate(features):\n                if j <= i or j in used:\n                    continue\n                \n                layer_j = feat_j['layer']\n                idx_j = feat_j['index']\n                \n                dec_j = torch.zeros(self.d_model)\n                for l in range(layer_j, self.n_layers):\n                    dec_j += self.clt.decoders[f\"{layer_j}_to_{l}\"].weight[:, idx_j]\n                \n                similarity = F.cosine_similarity(dec_i.unsqueeze(0), dec_j.unsqueeze(0)).item()\n                \n                if similarity > similarity_threshold:\n                    group.append(feat_j)\n                    used.add(j)\n            \n            if len(group) > 1:\n                groups.append(group)\n        \n        supernode_id = 0\n        supernode_map = {}\n        supernodes = []\n        \n        for group in groups:\n            sn_name = f\"SN{supernode_id}\"\n            supernodes.append({\n                'id': sn_name,\n                'features': group,\n                'size': len(group),\n                'layers': list(set(f['layer'] for f in group))\n            })\n            \n            for feat in group:\n                orig_name = f\"L{feat['layer']}_F{feat['index']}\"\n                supernode_map[orig_name] = sn_name\n            \n            supernode_id += 1\n        \n        for feat in features:\n            orig_name = f\"L{feat['layer']}_F{feat['index']}\"\n            if orig_name not in supernode_map:\n                sn_name = f\"SN{supernode_id}\"\n                supernodes.append({\n                    'id': sn_name,\n                    'features': [feat],\n                    'size': 1,\n                    'layers': [feat['layer']]\n                })\n                supernode_map[orig_name] = sn_name\n                supernode_id += 1\n        \n        supernode_edges = {}\n        for edge in graph['edges']:\n            source = supernode_map.get(edge['source'], edge['source'])\n            target = supernode_map.get(edge['target'], edge['target'])\n            \n            key = (source, target, edge['type'])\n            if key not in supernode_edges:\n                supernode_edges[key] = []\n            supernode_edges[key].append(edge['weight'])\n        \n        aggregated_edges = []\n        for (source, target, edge_type), weights in supernode_edges.items():\n            total_weight = sum(weights)\n            frac_external = 1.0\n            \n            if abs(total_weight * frac_external) > 0.01:\n                aggregated_edges.append({\n                    'source': source,\n                    'target': target,\n                    'weight': total_weight * frac_external,\n                    'type': edge_type,\n                    'count': len(weights)\n                })\n        \n        return {\n            'supernodes': supernodes,\n            'edges': aggregated_edges,\n            'target_token': graph['target_token'],\n            'position': graph['position'],\n            'original_graph': graph\n        }\n    \n    def print_summary(self, graph: Dict):\n        print(f\"\\n=== Attribution Graph: Token {graph['target_token']} @ pos {graph['position']} ===\")\n        print(f\"Nodes: {len(graph['nodes']['embeddings'])} emb, {len(graph['nodes']['features'])} feat, \"\n              f\"{len(graph['nodes']['attention_heads'])} attn, {len(graph['nodes']['errors'])} err\")\n        \n        edge_types = {}\n        for edge in graph['edges']:\n            edge_types[edge['type']] = edge_types.get(edge['type'], 0) + 1\n        print(f\"Edges: {', '.join(f'{t}:{c}' for t, c in edge_types.items())}\")\n        \n        top_edges = sorted(graph['edges'], key=lambda e: abs(e['weight']), reverse=True)[:5]\n        print(\"Top edges:\")\n        for e in top_edges:\n            print(f\" {e['source']}->{e['target']}: {e['weight']:.3f} ({e['type']})\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "735c8f24-f561-4e63-a2fd-74aa07a33b1d",
   "metadata": {
    "language": "python",
    "name": "cell5",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "import torch\nimport numpy as np\nimport time\nfrom snowflake.snowpark.context import get_active_session\n\ndef get_pile_data(session, max_samples=500, seq_length=32, vocab_size=256):\n    \"\"\"Load and preprocess The Pile dataset from Snowflake stage\"\"\"\n    print(\"Loading The Pile from Snowflake stage...\")\n    \n    parquet_df = session.read.option(\"FILE_FORMAT\", \"MY_PILE_DATABASE.PUBLIC.PARQUET_FORMAT\") \\\n        .parquet(\"@MY_PILE_DATABASE.PUBLIC.MY_PILE_STAGE/train-00000-of-00002-9f1d227dc3989035.parquet\")\n    \n    # Use quoted identifier '\"text\"' to match Parquet column name\n    pandas_df = parquet_df.select('\"text\"').to_pandas()\n    \n    data = []\n    for i, row in pandas_df.iterrows():\n        if i >= max_samples:\n            break\n        \n        text = row['text']  # Column name in pandas_df is 'text' (unquoted)\n        if not text or not isinstance(text, str):\n            continue\n        \n        # Skip email headers if present\n        text = text.split('\\n\\n', 1)[-1] if '\\n\\n' in text else text\n        tokens = [min(ord(c) % vocab_size, vocab_size-1) for c in text[:seq_length*3]]\n        \n        if len(tokens) >= seq_length:\n            tokens = np.array(tokens[:seq_length*2])\n            shuffled_indices = np.random.permutation(len(tokens))\n            tokens = tokens[shuffled_indices][:seq_length]\n            data.append(torch.tensor(tokens, dtype=torch.long))\n        \n        if len(data) % 100 == 0 and len(data) > 0:\n            print(f\" Loaded {len(data)} samples\")\n    \n    if not data:\n        raise ValueError(\"No valid samples loaded from dataset\")\n    \n    return torch.stack(data)\n\ndef train_and_evaluate():\n    session = get_active_session()\n    \n    # Set database to MY_PILE_DATABASE\n    session.sql(\"USE DATABASE MY_PILE_DATABASE\").collect()\n    \n    device = torch.device('cpu')\n    config = MinimalConfig()\n    \n    print(f\"Config: {config.n_layer}L, {config.n_features}F, {config.n_embd}d\")\n    print(f\"Target training tokens (scaled for CPU): {config.n_training_tokens:,}\")\n    \n    base_model = MinimalTransformer(config).to(device)\n    clt = CrossLayerTranscoder(config).to(device)\n    \n    data = get_pile_data(session, max_samples=500, seq_length=config.block_size)\n    data = data.to(device)\n    print(f\"Dataset: {len(data)} samples\")\n    \n    trainer = CLTTrainer(base_model, clt, config)\n    batch_size = 16\n    start_time = time.time()\n    \n    print(\"\\nTraining CLT...\")\n    print(f\"Total training steps: {trainer.total_steps}\")\n    \n    tokens_seen = 0\n    for epoch in range(5):\n        for i in range(0, len(data), batch_size):\n            batch = data[i:i+batch_size]\n            if batch.shape[0] < batch_size:\n                continue\n            \n            stats = trainer.train_step(batch)\n            tokens_seen += batch_size * config.block_size\n            \n            if stats['l0'] > 50 and stats['l0'] < 150 and stats['recon'] < 0.1:\n                print(f\"Early stop: L0={stats['l0']:.1f}, Recon={stats['recon']:.4f}\")\n                break\n            \n            if trainer.step % 50 == 0:\n                print(f\"Step {trainer.step}/{trainer.total_steps}: \"\n                      f\"Loss={stats['loss']:.4f}, L0={stats['l0']:.1f}, \"\n                      f\"Recon={stats['recon']:.4f}, Tokens={tokens_seen:,}\")\n            \n            if trainer.step >= trainer.total_steps:\n                break\n        \n        if trainer.step >= trainer.total_steps:\n            break\n    \n    print(f\"\\nTraining completed in {time.time() - start_time:.1f}s\")\n    print(f\"Total tokens seen: {tokens_seen:,}\")\n    \n    print(\"\\nComputing Attribution Graphs...\")\n    graph_builder = AttributionGraph(base_model, clt)\n    \n    for i in range(min(3, len(data))):\n        test_input = data[i:i+1]\n        graph = graph_builder.compute_full_graph(test_input, target_pos=-1, use_twera=False)\n        print(f\"\\n--- Sample {i+1} (Full Graph) ---\")\n        graph_builder.print_summary(graph)\n        \n        pruned_graph = graph_builder.prune_graph(graph, node_threshold=0.8, edge_threshold=0.98)\n        print(f\"\\n--- Sample {i+1} (Pruned Graph, 80% nodes, 98% edges) ---\")\n        graph_builder.print_summary(pruned_graph)\n        \n        supernode_graph = graph_builder.group_into_supernodes(pruned_graph, similarity_threshold=0.7)\n        print(f\"\\n--- Sample {i+1} (Supernode Graph) ---\")\n        print(f\"Supernodes: {len(supernode_graph['supernodes'])}\")\n        for sn in supernode_graph['supernodes'][:3]:\n            print(f\" {sn['id']}: {sn['size']} features from layers {sn['layers']}\")\n        print(f\"Aggregated edges: {len(supernode_graph['edges'])}\")\n        top_sn_edges = sorted(supernode_graph['edges'], key=lambda e: abs(e['weight']), reverse=True)[:3]\n        for e in top_sn_edges:\n            print(f\" {e['source']}->{e['target']}: {e['weight']:.3f} ({e['count']} edges)\")\n        \n        if len(graph_builder.coactivation_counts) > 10:\n            graph_twera = graph_builder.compute_full_graph(test_input, target_pos=-1, use_twera=True)\n            print(f\"\\n--- Sample {i+1} (With TWERA filtering) ---\")\n            graph_builder.print_summary(graph_twera)\n    \n    print(f\"\\nTotal runtime: {time.time() - start_time:.1f}s\")\n    return base_model, clt, graph_builder\n\nif __name__ == \"__main__\":\n    # Install required packages\n    try:\n        import torch\n        import numpy\n        import snowflake.snowpark\n    except ImportError:\n        !pip install torch numpy snowflake-snowpark-python\n    \n    base_model, clt, graph_builder = train_and_evaluate()\n    ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a66fa355-91fa-40f9-bce2-9f3b35e54381",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  }
 ]
}